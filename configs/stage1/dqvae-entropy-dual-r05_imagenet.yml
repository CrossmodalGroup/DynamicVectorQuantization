model:
  base_learning_rate: 4.5e-06
  target: models.stage1_dynamic.dqvae_dual_entropy.DualGrainVQModel
  params:
    encoderconfig:
      target: modules.dynamic_modules.EncoderDual.DualGrainEncoder
      params:
        ch: 128
        ch_mult: [1,1,2,2,4]
        num_res_blocks: 2
        attn_resolutions: [16, 32]
        dropout: 0.0
        resamp_with_conv: true
        in_channels: 3
        resolution: 256
        z_channels: 256
        update_router: False
        router_config:
          target: modules.dynamic_modules.RouterDual.DualGrainFixedEntropyRouter
          params:
            json_path: scripts/tools/thresholds/entropy_thresholds_imagenet_train_patch-16.json
            fine_grain_ratito: 0.5
    decoderconfig:
      target: modules.dynamic_modules.DecoderPositional.Decoder
      params:
        ch: 128
        in_ch: 256
        out_ch: 3
        ch_mult: [1,1,2,2]
        num_res_blocks: 2
        resolution: 256
        attn_resolutions: [32]
        latent_size: 32
        window_size: 2
        position_type: fourier+learned
    lossconfig:
      target: modules.losses.vqperceptual_multidisc.VQLPIPSWithDiscriminator
      params:
        disc_start: 0
        disc_config:
          target: modules.discriminator.model.NLayerDiscriminator
          params:
            input_nc: 3
            ndf: 64
            n_layers: 3
            use_actnorm: false
        disc_init: true
        codebook_weight: 1.0
        pixelloss_weight: 1.0
        disc_factor: 1.0
        disc_weight: 1.0
        perceptual_weight: 1.0
        disc_conditional: false
        disc_loss: hinge
        disc_weight_max: 0.75
        # budget_loss_config:
        #   target: modules.dynamic_modules.budget.BudgetConstraint_RatioMSE_DualGrain
        #   params:
        #     target_ratio: 0.5
        #     gamma: 10.0
        #     min_grain_size: 16
        #     max_grain_size: 32
        #     calculate_all: True
    vqconfig:
      target: modules.vector_quantization.quantize2_mask.VectorQuantize2
      # target: modules.vector_quantization.quantize_codebook_mask.MaskVectorQuantize
      params:
        codebook_size: 1024
        codebook_dim: 256
        channel_last: false
        accept_image_fmap: true
        commitment_beta: 0.25
        decay: 0.99
        restart_unused_codes: True
    quant_before_dim: 256
    quant_after_dim: 256
    quant_sample_temperature: 0.0
    image_key: image
    monitor: val_rec_loss
    warmup_epochs: 0.1
    scheduler_type: linear-warmup_cosine-decay

data:
  target: data.build.DataModuleFromConfig
  params:
    batch_size: 30
    num_workers: 8
    train:
      target: data.imagenet.ImageNetTrain
      params:
        config:
          is_eval: False
          size: 256
    validation:
      target: data.imagenet.ImageNetValidation
      params:
        config:
          is_eval: True
          size: 256
